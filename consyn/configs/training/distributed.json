{
  "distributed": false,
  "precision": "fp16",
  "optimizer": {
      "type": "adamw",
      "learning_rate": 5e-5,
      "weight_decay": 0.01,
      "beta1": 0.9,
      "beta2": 0.999,
      "epsilon": 1e-8
  },
  "scheduler": {
      "type": "cosine",
      "warmup_ratio": 0.1,
      "min_learning_rate_ratio": 0.1
  },
  "gradient_accumulation": {
      "enabled": true,
      "steps": 1
  },
  "mixed_precision": {
      "enabled": true,
      "dtype": "float16"
  }
}